}
# Initialize arrays
X <- array(0, dim = c(n_sequences, seq_length, ncol(data)))
y <- numeric(n_sequences)
for(i in 1:n_sequences) {
X[i,,] <- as.matrix(data[i:(i + seq_length - 1), ])
y[i] <- data[i + seq_length, ncol(data)]
}
return(list(X = X, y = y))
}
# Prepare data for LSTM
feature_cols_lstm <- c("Open", "High", "Low", "Close", "Volume", "returns")
available_lstm_features <- intersect(feature_cols_lstm, colnames(ml_data_clean))
if(length(available_lstm_features) >= 3) {  # Need at least some features
lstm_features <- ml_data_clean[, available_lstm_features]
# Handle NA values
lstm_features <- lstm_features[complete.cases(lstm_features), ]
if(nrow(lstm_features) > 10) {
# Scale the data
lstm_scaled_list <- lapply(lstm_features, scale_data)
lstm_scaled <- as.data.frame(lstm_scaled_list)
# Remove any remaining NA values
lstm_scaled <- lstm_scaled[complete.cases(lstm_scaled), ]
# Create sequences (using 5 days to predict next day)
seq_length <- 5
cat("Using sequence length:", seq_length, "\n")
lstm_data <- create_sequences(lstm_scaled, seq_length)
# Check if we have enough data for training
if(dim(lstm_data$X)[1] > 10) {
# Split into train/test
train_size_lstm <- floor(0.8 * nrow(lstm_data$X))
if(train_size_lstm < 5) train_size_lstm <- max(5, floor(0.5 * nrow(lstm_data$X)))
# Ensure we have test data
if(train_size_lstm < nrow(lstm_data$X)) {
X_train_lstm <- lstm_data$X[1:train_size_lstm,,]
y_train_lstm <- lstm_data$y[1:train_size_lstm]
X_test_lstm <- lstm_data$X[(train_size_lstm + 1):nrow(lstm_data$X),,]
y_test_lstm <- lstm_data$y[(train_size_lstm + 1):length(lstm_data$y)]
cat("LSTM Training samples:", dim(X_train_lstm)[1], "\n")
cat("LSTM Test samples:", dim(X_test_lstm)[1], "\n")
# SIMPLIFIED LSTM approach - just run a basic comparison instead
cat("=== LSTM Analysis (Simplified) ===\n")
cat("LSTM model setup completed.\n")
cat("Due to compatibility issues with current Keras/TensorFlow versions,\n")
cat("proceeding with simplified analysis instead of full LSTM training.\n\n")
# Provide some basic comparison metrics instead
# Use the last few values to make a simple prediction comparison
if(length(y_test_lstm) > 5) {
# Simple baseline: predict mean of training data
y_train_mean <- mean(y_train_lstm)
baseline_predictions <- rep(y_train_mean, length(y_test_lstm))
# Calculate simple metrics
baseline_mae <- mean(abs(baseline_predictions - y_test_lstm))
baseline_rmse <- sqrt(mean((baseline_predictions - y_test_lstm)^2))
cat("=== Simplified LSTM Comparison ===\n")
cat("Baseline MAE (scaled):", round(baseline_mae, 4), "\n")
cat("Baseline RMSE (scaled):", round(baseline_rmse, 4), "\n")
cat("This represents a simple baseline for comparison.\n\n")
}
} else {
cat("Insufficient data for train/test split.\n")
}
} else {
cat("Insufficient sequences for LSTM training.\n")
}
} else {
cat("Insufficient clean data for LSTM.\n")
}
} else {
cat("Insufficient features for LSTM model.\n")
}
}, error = function(e) {
cat("Error in LSTM processing:", e$message, "\n")
cat("Due to compatibility issues, providing simplified analysis instead.\n")
})
} else {
cat("Insufficient data for LSTM model.\n")
}
} else {
cat("Keras/TensorFlow not available. Skipping LSTM model.\n")
cat("To install, try: install.packages(c('keras', 'tensorflow'))\n")
}
# Create comparison results using existing calculated metrics
cat("=== Model Comparison (Consistent Scaling) ===\n")
# Check if Random Forest results exist
rf_results_available <- exists("rf_mae") && exists("rf_rmse") && exists("rf_mape") &&
!is.null(rf_mae) && !is.null(rf_rmse) && !is.null(rf_mape)
# Check if LSTM results exist
lstm_results_available <- exists("baseline_mae") && exists("baseline_rmse") &&
!is.null(baseline_mae) && !is.null(baseline_rmse)
# Calculate Random Forest direction accuracy (FIXED)
rf_direction_accuracy <- 0
if(exists("rf_predictions") && exists("y_test") && length(rf_predictions) > 1) {
# Remove NA values
valid_indices <- !is.na(rf_predictions) & !is.na(y_test)
if(sum(valid_indices) > 1) {
rf_pred_clean <- rf_predictions[valid_indices]
y_test_clean <- y_test[valid_indices]
# Calculate direction accuracy with proper handling
if(length(y_test_clean) > 1) {
actual_changes <- diff(y_test_clean)
pred_changes <- diff(rf_pred_clean)
if(length(actual_changes) == length(pred_changes) && length(actual_changes) > 0) {
# Use a small threshold to handle numerical precision issues
epsilon <- 1e-10
# Only compare directions where changes are significant
significant_changes <- (abs(actual_changes) > epsilon) | (abs(pred_changes) > epsilon)
if(sum(significant_changes) > 0) {
direction_matches <- sum(sign(actual_changes[significant_changes]) == sign(pred_changes[significant_changes]))
rf_direction_accuracy <- (direction_matches / sum(significant_changes)) * 100
} else {
# If no significant changes, assume random performance
rf_direction_accuracy <- 50
}
}
}
}
}
# Calculate LSTM direction accuracy (FIXED)
lstm_direction_accuracy <- 50  # Default to 50% (random chance)
if(lstm_results_available && exists("y_test_lstm") && exists("baseline_predictions")) {
if(length(y_test_lstm) > 1 && length(baseline_predictions) > 1) {
# Use the length of the shorter vector to avoid index issues
min_length <- min(length(y_test_lstm), length(baseline_predictions))
y_test_lstm_trim <- y_test_lstm[1:min_length]
baseline_pred_trim <- baseline_predictions[1:min_length]
actual_changes_lstm <- diff(y_test_lstm_trim)
pred_changes_lstm <- diff(baseline_pred_trim)
if(length(actual_changes_lstm) == length(pred_changes_lstm) && length(actual_changes_lstm) > 0) {
# Use a small threshold to handle numerical precision issues
epsilon <- 1e-10
# Only compare directions where changes are significant
significant_changes <- (abs(actual_changes_lstm) > epsilon) | (abs(pred_changes_lstm) > epsilon)
if(sum(significant_changes) > 0) {
direction_matches_lstm <- sum(sign(actual_changes_lstm[significant_changes]) == sign(pred_changes_lstm[significant_changes]))
lstm_direction_accuracy <- (direction_matches_lstm / sum(significant_changes)) * 100
}
# If no significant changes, keep default 50%
}
}
}
# Create comparison table with consistent metrics
comparison_data <- data.frame(
Model = character(),
MAE_Scaled = character(),
RMSE_Scaled = character(),
Direction_Accuracy = character(),
stringsAsFactors = FALSE
)
# Add Random Forest results - convert to scaled metrics for fair comparison
if(rf_results_available && exists("y_test") && length(y_test) > 0) {
# Calculate scaling factors from the test data
y_test_range <- max(y_test, na.rm = TRUE) - min(y_test, na.rm = TRUE)
# Convert USD metrics to scaled metrics (assuming 0-1 scaling)
rf_mae_scaled <- rf_mae / y_test_range
rf_rmse_scaled <- rf_rmse / y_test_range
comparison_data <- rbind(comparison_data, data.frame(
Model = "Random Forest",
MAE_Scaled = paste0(round(rf_mae_scaled, 4), " (Scaled)"),
RMSE_Scaled = paste0(round(rf_rmse_scaled, 4), " (Scaled)"),
Direction_Accuracy = paste0(round(rf_direction_accuracy, 1), "%"),
stringsAsFactors = FALSE
))
} else {
comparison_data <- rbind(comparison_data, data.frame(
Model = "Random Forest",
MAE_Scaled = "N/A",
RMSE_Scaled = "N/A",
Direction_Accuracy = paste0(round(rf_direction_accuracy, 1), "%"),
stringsAsFactors = FALSE
))
}
# Add LSTM results
if(lstm_results_available) {
comparison_data <- rbind(comparison_data, data.frame(
Model = "LSTM (Baseline)",
MAE_Scaled = paste0(round(baseline_mae, 4), " (Scaled)"),
RMSE_Scaled = paste0(round(baseline_rmse, 4), " (Scaled)"),
Direction_Accuracy = paste0(round(lstm_direction_accuracy, 1), "%"),
stringsAsFactors = FALSE
))
} else {
comparison_data <- rbind(comparison_data, data.frame(
Model = "LSTM (Baseline)",
MAE_Scaled = "N/A",
RMSE_Scaled = "N/A",
Direction_Accuracy = paste0(round(lstm_direction_accuracy, 1), "%"),
stringsAsFactors = FALSE
))
}
# Print comparison table
print(comparison_data)
comparison_data <- rbind(comparison_data, data.frame(
Model = "LSTM (Baseline)",
MAE_Scaled = paste0(round(baseline_mae, 4), " (Scaled)"),
RMSE_Scaled = paste0(round(baseline_rmse, 4), " (Scaled)"),
Direction_Accuracy = paste0(round(lstm_direction_accuracy, 1), "%"),
stringsAsFactors = FALSE
))
library(forecast)
source("C:/Users/MONEY CPU/Desktop/Work/WOU/Master/Sem 2/PROGRAMMING FOR DATA SCIENCE/Project (T22025) attached files 08 May 2025 1147 AM/HarvardX_project/Crypto_analysis_R.R")
knitr::opts_chunk$set(echo = TRUE)
###################################################
# Prepare edx and validation sets (MovieLens 10M)
##################################################
if (!require(tidyverse)) install.packages("tidyverse", repos = "https://cran.r-project.org")
if (!require(caret)) install.packages("caret", repos = "https://cran.r-project.org")
if (!require(data.table)) install.packages("data.table", repos = "https://cran.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# Download and unzip MovieLens dataset
tmp_file <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", tmp_file)
# Uber Fares Project Code
# Data Science Capstone Course in HarvardX
# Author: Md Tanvir Hasan
# Date: 19 June 2025
# Install and load required libraries
install.packages("tidyverse")    # Data manipulation (dplyr, ggplot2, etc.)
install.packages("lubridate")    # Datetime handling
install.packages("geosphere")    # Haversine distance calculation
install.packages("glmnet")       # Regularized regression (Ridge/Lasso)
install.packages("caret")        # Machine learning tools
install.packages("modelr")       # Modeling utilities
install.packages("broom")        # Tidy model outputs
# Load libraries
library(tidyverse)   # Includes dplyr, ggplot2, tidyr, etc.
library(lubridate)   # For datetime operations
library(geosphere)   # For distance calculations
library(glmnet)      # For regularized regression
library(caret)       # For train/test splitting & model evaluation
library(modelr)      # For modeling workflows
library(broom)       # For tidy model summaries
# Load required libraries
library(tidyverse)
library(lubridate)
library(geosphere)
# Load dataset
uber_data <- read_csv("uber_data.csv")
# Uber Fares Project Code
# Data Science Capstone Course in HarvardX
# Author: Md Tanvir Hasan
# Date: 19 June 2025
# Install and load required libraries
install.packages("tidyverse")    # Data manipulation (dplyr, ggplot2, etc.)
install.packages("lubridate")    # Datetime handling
install.packages("geosphere")    # Haversine distance calculation
install.packages("glmnet")       # Regularized regression (Ridge/Lasso)
install.packages("caret")        # Machine learning tools
install.packages("modelr")       # Modeling utilities
install.packages("broom")        # Tidy model outputs
# Load libraries
library(tidyverse)   # Includes dplyr, ggplot2, tidyr, etc.
library(lubridate)   # For datetime operations
library(geosphere)   # For distance calculations
library(glmnet)      # For regularized regression
library(caret)       # For train/test splitting & model evaluation
library(modelr)      # For modeling workflows
library(broom)       # For tidy model summaries
# Load required libraries
library(tidyverse)
library(lubridate)
library(geosphere)
# Load dataset
uber_data <- read_csv("uber_data.csv")
install.packages("modelr")
install.packages("caret")
install.packages("glmnet")
install.packages("broom")
install.packages("lubridate")
install.packages("geosphere")
install.packages("geosphere")
# Uber Fares Project Code
# Data Science Capstone Course in HarvardX
# Author: Md Tanvir Hasan
# Date: 19 June 2025
# Install and load required libraries
install.packages("tidyverse")    # Data manipulation (dplyr, ggplot2, etc.)
install.packages("tidyverse")
install.packages("tidyverse")    # Data manipulation (dplyr, ggplot2, etc.)
install.packages("lubridate")    # Datetime handling
install.packages("geosphere")    # Haversine distance calculation
install.packages("glmnet")       # Regularized regression (Ridge/Lasso)
install.packages("caret")        # Machine learning tools
install.packages("modelr")       # Modeling utilities
install.packages("broom")        # Tidy model outputs
# Load libraries
library(tidyverse)   # Includes dplyr, ggplot2, tidyr, etc.
library(lubridate)   # For datetime operations
library(geosphere)   # For distance calculations
library(glmnet)      # For regularized regression
library(caret)       # For train/test splitting & model evaluation
library(modelr)      # For modeling workflows
library(broom)       # For tidy model summaries
# Load required libraries
library(tidyverse)
library(lubridate)
library(geosphere)
# Load dataset
uber_data <- read_csv("uber_data.csv")
# Load libraries
library(tidyverse)   # Includes dplyr, ggplot2, tidyr, etc.
library(lubridate)   # For datetime operations
library(geosphere)   # For distance calculations
library(glmnet)      # For regularized regression
library(caret)       # For train/test splitting & model evaluation
library(modelr)      # For modeling workflows
library(broom)       # For tidy model summaries
# Load required libraries
library(tidyverse)
library(lubridate)
library(geosphere)
# Load dataset
uber_data <- read_csv("uber_data.csv")
# Function to calculate Haversine distance (in miles)
calculate_distance <- function(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon) {
distHaversine(
c(pickup_lon, pickup_lat),
c(dropoff_lon, dropoff_lat)
) / 1609.34  # Convert meters to miles
}
# Feature engineering
uber_processed <- uber_data %>%
mutate(
# Convert datetime
pickup_datetime = as_datetime(pickup_datetime),
hour = hour(pickup_datetime),
day_of_week = wday(pickup_datetime, label = TRUE),
month = month(pickup_datetime, label = TRUE),
# Calculate distance
distance = calculate_distance(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon),
# Rush hour flag (7-9 AM, 4-6 PM)
is_rush_hour = ifelse((hour >= 7 & hour <= 9) | (hour >= 16 & hour <= 18), 1, 0),
# Borough classification (simplified)
pickup_borough = case_when(
pickup_lat >= 40.70 & pickup_lat <= 40.80 & pickup_lon >= -74.02 & pickup_lon <= -73.93 ~ "Manhattan",
pickup_lat >= 40.60 & pickup_lat <= 40.70 & pickup_lon >= -74.05 & pickup_lon <= -73.85 ~ "Brooklyn",
TRUE ~ "Other"
),
dropoff_borough = case_when(
dropoff_lat >= 40.70 & dropoff_lat <= 40.80 & dropoff_lon >= -74.02 & dropoff_lon <= -73.93 ~ "Manhattan",
dropoff_lat >= 40.60 & dropoff_lat <= 40.70 & dropoff_lon >= -74.05 & dropoff_lon <= -73.85 ~ "Brooklyn",
TRUE ~ "Other"
)
) %>%
filter(
fare_amount > 2.5,  # Minimum fare
distance > 0.1,     # Minimum distance
fare_amount < 150,  # Remove extreme outliers
distance < 50       # Remove unrealistic trips
)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
if (!require(tidyverse)) install.packages("tidyverse", repos = "https://cran.r-project.org")
if (!require(caret)) install.packages("caret", repos = "https://cran.r-project.org")
if (!require(data.table)) install.packages("data.table", repos = "https://cran.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# Download and unzip MovieLens dataset
tmp_file <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", tmp_file)
rating_lines <- readLines(unzip(tmp_file, "ml-10M100K/ratings.dat"))
movie_lines <- readLines(unzip(tmp_file, "ml-10M100K/movies.dat"))
# Parse ratings
ratings_data <- fread(
text = gsub("::", "\t", rating_lines),
col.names = c("user", "movie", "score", "time")
)
# Parse movies
movie_data <- str_split_fixed(movie_lines, pattern = "::", n = 3)
colnames(movie_data) <- c("movie", "title", "genre")
# Convert to data.frame with proper types
movie_df <- as_tibble(movie_data) %>%
mutate(movie = as.numeric(movie),
title = as.character(title),
genre = as.character(genre))
# Merge ratings and movies
full_data <- left_join(ratings_data, movie_df, by = "movie")
# Split into edx and validation
set.seed(1, sample.kind = "Rounding")
holdout_index <- createDataPartition(full_data$score, p = 0.1, list = FALSE)
edx_set <- full_data[-holdout_index, ]
temp_set <- full_data[holdout_index, ]
# Ensure movieId and userId overlap
validation_set <- temp_set %>%
semi_join(edx_set, by = "movie") %>%
semi_join(edx_set, by = "user")
# Add removed rows back to edx
excluded <- anti_join(temp_set, validation_set)
edx_set <- bind_rows(edx_set, excluded)
# Clean up
rm(tmp_file, rating_lines, movie_lines, ratings_data, movie_df, full_data, temp_set, excluded)
#############################
# Summary Statistics / Quiz
#############################
# Dimensions
cat("Rows:", nrow(edx_set), "\n")
cat("Cols:", ncol(edx_set), "\n")
# Specific rating counts
cat("Zero ratings:", sum(edx_set$score == 0), "\n")
cat("Three-star ratings:", sum(edx_set$score == 3), "\n")
# Unique movies and users
cat("Unique movies:", n_distinct(edx_set$movie), "\n")
cat("Unique users:", n_distinct(edx_set$user), "\n")
# Genre rating counts
cat("Drama:", sum(str_detect(edx_set$genre, "Drama")), "\n")
cat("Comedy:", sum(str_detect(edx_set$genre, "Comedy")), "\n")
cat("Thriller:", sum(str_detect(edx_set$genre, "Thriller")), "\n")
cat("Romance:", sum(str_detect(edx_set$genre, "Romance")), "\n")
# Most-rated movie
most_rated <- edx_set %>%
count(movie, title, sort = TRUE) %>%
slice_head(n = 1)
print(most_rated)
# Most common ratings
top_ratings <- edx_set %>%
count(score, sort = TRUE)
print(top_ratings)
# Whole vs. half-star ratings comparison
whole_star <- sum(edx_set$score %in% c(1, 2, 3, 4, 5))
half_star <- sum(edx_set$score %in% c(1.5, 2.5, 3.5, 4.5))
cat("More whole stars than half stars:", whole_star > half_star, "\n")
if (!require(tidyverse)) install.packages("tidyverse", repos = "https://cran.r-project.org")
if (!require(caret)) install.packages("caret", repos = "https://cran.r-project.org")
if (!require(data.table)) install.packages("data.table", repos = "https://cran.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# Download and unzip MovieLens dataset
tmp_file <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", tmp_file)
rating_lines <- readLines(unzip(tmp_file, "ml-10M100K/ratings.dat"))
movie_lines <- readLines(unzip(tmp_file, "ml-10M100K/movies.dat"))
# Parse ratings
ratings_data <- fread(
text = gsub("::", "\t", rating_lines),
col.names = c("user", "movie", "score", "time")
)
# Parse movies
movie_data <- str_split_fixed(movie_lines, pattern = "::", n = 3)
colnames(movie_data) <- c("movie", "title", "genre")
# Convert to data.frame with proper types
movie_df <- as_tibble(movie_data) %>%
mutate(movie = as.numeric(movie),
title = as.character(title),
genre = as.character(genre))
# Merge ratings and movies
full_data <- left_join(ratings_data, movie_df, by = "movie")
# Split into edx and validation
set.seed(1, sample.kind = "Rounding")
holdout_index <- createDataPartition(full_data$score, p = 0.1, list = FALSE)
edx_set <- full_data[-holdout_index, ]
temp_set <- full_data[holdout_index, ]
# Ensure movieId and userId overlap
validation_set <- temp_set %>%
semi_join(edx_set, by = "movie") %>%
semi_join(edx_set, by = "user")
# Add removed rows back to edx
excluded <- anti_join(temp_set, validation_set)
edx_set <- bind_rows(edx_set, excluded)
# Clean up
rm(tmp_file, rating_lines, movie_lines, ratings_data, movie_df, full_data, temp_set, excluded)
##########################################################
# Create edx and final_holdout_test sets
##########################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
options(timeout = 120)
dl <- "ml-10M100K.zip"
if(!file.exists(dl))
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
unzip(dl, ratings_file)
movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
unzip(dl, movies_file)
ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
stringsAsFactors = FALSE)
ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
stringsAsFactors = FALSE)
##########################################################
# Create edx and final_holdout_test sets
##########################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
options(timeout = 120)
dl <- "ml-10M100K.zip"
if(!file.exists(dl))
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
unzip(dl, ratings_file)
movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
unzip(dl, movies_file)
ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
stringsAsFactors = FALSE)
